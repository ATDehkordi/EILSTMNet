{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please remind that this code is not the final code of the paper. The final code will be released upon acceptance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part is used to prepare the input data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "n = 30  # length of timeseries window (days)\n",
    "\n",
    "# Map each \"_befor.xlsx\" file to its corresponding environmental file\n",
    "PAIRINGS = {\n",
    "    \"M_befor.xlsx\":  \"W and M.xlsx\",\n",
    "    \"W_befor.xlsx\":  \"W and M.xlsx\",\n",
    "    \"O1_befor.xlsx\": \"O1 and O2.xlsx\",\n",
    "    \"O2_befor.xlsx\": \"O1 and O2.xlsx\",\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# HELPERS\n",
    "# -----------------------------\n",
    "def map_sensor(sensor_name: str) -> int:\n",
    "    if isinstance(sensor_name, str):\n",
    "        if sensor_name.startswith(\"S3A\"):\n",
    "            return 0\n",
    "        elif sensor_name.startswith(\"S3B\"):\n",
    "            return 1\n",
    "        elif sensor_name.startswith(\"S6\"):\n",
    "            return 2\n",
    "        elif sensor_name.startswith(\"SWOT\"):\n",
    "            return 3\n",
    "    return -1  # Unknown sensor\n",
    "\n",
    "def prepare_weather_df(env_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and index the environmental dataframe by DATE (datetime).\"\"\"\n",
    "    df_weather = pd.read_excel(env_path)\n",
    "    # Expecting a DATE column plus Temp_Celsius, Prec, Evap\n",
    "    if \"DATE\" not in df_weather.columns:\n",
    "        raise ValueError(f\"Expected 'DATE' column in {env_path}. Found: {list(df_weather.columns)}\")\n",
    "\n",
    "    df_weather[\"DATE\"] = pd.to_datetime(df_weather[\"DATE\"])\n",
    "    df_weather = df_weather.sort_values(\"DATE\").reset_index(drop=True).set_index(\"DATE\")\n",
    "    for needed in [\"Temp_Celsius\", \"Prec\", \"Evap\"]:\n",
    "        if needed not in df_weather.columns:\n",
    "            raise ValueError(f\"Missing column '{needed}' in {env_path}.\")\n",
    "    return df_weather\n",
    "\n",
    "def process_one(before_path: str, env_path: str, n_days: int) -> pd.DataFrame:\n",
    "    \"\"\"Process a single pair: before file + env file, return the valid rows dataframe.\"\"\"\n",
    "    # Load the 'before' dataframe\n",
    "    df_alt = pd.read_excel(before_path)  # expects columns: DateTime, Predicted_WSE, Sensor, (and In_Situ_WSE)\n",
    "    if \"DateTime\" not in df_alt.columns:\n",
    "        raise ValueError(f\"Expected 'DateTime' column in {before_path}. Found: {list(df_alt.columns)}\")\n",
    "\n",
    "    # Convert DateTime to datetime format\n",
    "    df_alt[\"DateTime\"] = pd.to_datetime(df_alt[\"DateTime\"])\n",
    "\n",
    "    # Time features\n",
    "    df_alt[\"Year\"] = df_alt[\"DateTime\"].dt.year\n",
    "    df_alt[\"DOY\"] = df_alt[\"DateTime\"].dt.dayofyear\n",
    "    df_alt[\"DOY_sin\"] = np.sin(2 * np.pi * df_alt[\"DOY\"] / 365)\n",
    "    df_alt[\"DOY_cos\"] = np.cos(2 * np.pi * df_alt[\"DOY\"] / 365)\n",
    "\n",
    "    # Sensor code\n",
    "    df_alt[\"Sensor_Code\"] = df_alt[\"Sensor\"].apply(map_sensor)\n",
    "\n",
    "    # Check unknown sensors\n",
    "    unknown_sensors = df_alt[df_alt[\"Sensor_Code\"] == -1]\n",
    "    if not unknown_sensors.empty:\n",
    "        print(f\"[WARN] Unrecognized sensors found in {before_path}:\")\n",
    "        print(unknown_sensors[[\"DateTime\", \"Sensor\"]].head(10))  # preview first 10\n",
    "\n",
    "    # Preserve In_Situ_WSE if present\n",
    "    if \"In_Situ_WSE\" in df_alt.columns:\n",
    "        df_alt[\"In_Situ_WSE\"] = df_alt[\"In_Situ_WSE\"]\n",
    "\n",
    "    # Weather\n",
    "    df_weather = prepare_weather_df(env_path)\n",
    "\n",
    "    # Build rolling windows\n",
    "    temp_series, prec_series, evap_series, valid_flags = [], [], [], []\n",
    "    # Normalize timestamps to date (midnight) and slice inclusive of the current date\n",
    "    for timestamp in df_alt[\"DateTime\"]:\n",
    "        start_date = timestamp.normalize() - pd.Timedelta(days=n_days - 1)\n",
    "        end_date = timestamp.normalize()\n",
    "        ts_slice = df_weather.loc[start_date:end_date]\n",
    "\n",
    "        if len(ts_slice) == n_days:\n",
    "            temp_series.append(ts_slice[\"Temp_Celsius\"].values.tolist())\n",
    "            prec_series.append(ts_slice[\"Prec\"].values.tolist())\n",
    "            evap_series.append(ts_slice[\"Evap\"].values.tolist())\n",
    "            valid_flags.append(True)\n",
    "        else:\n",
    "            temp_series.append(None)\n",
    "            prec_series.append(None)\n",
    "            evap_series.append(None)\n",
    "            valid_flags.append(False)\n",
    "\n",
    "    df_alt[\"Temp_series\"] = temp_series\n",
    "    df_alt[\"Prec_series\"] = prec_series\n",
    "    df_alt[\"Evap_series\"] = evap_series\n",
    "    df_alt[\"Valid\"] = valid_flags\n",
    "\n",
    "    # Keep only rows with complete history\n",
    "    df_valid = df_alt[df_alt[\"Valid\"] == True].reset_index(drop=True)\n",
    "    return df_valid\n",
    "\n",
    "# -----------------------------\n",
    "# RUN FOR ALL PAIRS & SAVE\n",
    "# -----------------------------\n",
    "all_outputs = []\n",
    "output_files = []\n",
    "\n",
    "for before_file, env_file in PAIRINGS.items():\n",
    "    if not Path(before_file).exists():\n",
    "        print(f\"[SKIP] {before_file} not found.\")\n",
    "        continue\n",
    "    if not Path(env_file).exists():\n",
    "        print(f\"[SKIP] Env file {env_file} not found for {before_file}.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {before_file} with {env_file} ...\")\n",
    "    df_valid = process_one(before_file, env_file, n)\n",
    "    all_outputs.append(df_valid)\n",
    "\n",
    "    # # Name output using the prefix (M, W, O1, O2)\n",
    "    # prefix = Path(before_file).stem.split(\"_\")[0]  # e.g., \"M\" from \"M_befor.xlsx\"\n",
    "    # out_name = f\"{prefix}_alt_ERA5_env_{n}days.xlsx\"\n",
    "    # df_valid.to_excel(out_name, index=False)\n",
    "    # output_files.append(out_name)\n",
    "    # print(f\"  -> Saved {out_name} (rows: {len(df_valid)})\")\n",
    "\n",
    "# Merge and save combined dataset\n",
    "if all_outputs:\n",
    "    df_merged = pd.concat(all_outputs, ignore_index=True)\n",
    "    merged_name = f\"ALL_alt_ERA5_env_{n}days.xlsx\"\n",
    "    df_merged.to_excel(merged_name, index=False)\n",
    "    print(f\"Saved merged file: {merged_name} (total rows: {len(df_merged)})\")\n",
    "else:\n",
    "    print(\"No outputs were generated. Check file paths and columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "df_data = pd.read_excel(f\"ALL_alt_ERA5_env_30days.xlsx\")   # columns: DateTime, Predicted_WSE, Sensor\n",
    "\n",
    "input_point_features_WSE = np.reshape(df_data['Predicted_WSE'].values, (-1,1))\n",
    "\n",
    "input_point_features_DOY = np.column_stack([\n",
    "    df_data['DOY_sin'].values,\n",
    "    df_data['DOY_cos'].values\n",
    "])\n",
    "\n",
    "input_point_features_sensorcodes = df_data['Sensor_Code'].values\n",
    "\n",
    "num_classes = input_point_features_sensorcodes.max() + 1\n",
    "input_point_features_sensorcodes = np.eye(num_classes)[input_point_features_sensorcodes]\n",
    "\n",
    "input_T = np.column_stack([\n",
    "    np.array([ast.literal_eval(s) for s in df_data['Temp_series'].values])\n",
    "])\n",
    "\n",
    "input_P = np.column_stack([\n",
    "    np.array([ast.literal_eval(s) for s in df_data['Prec_series'].values])\n",
    "])\n",
    "input_Ev = np.column_stack([\n",
    "    np.array([ast.literal_eval(s) for s in df_data['Evap_series'].values])\n",
    "])\n",
    "target = np.column_stack([\n",
    "    df_data['In_Situ_WSE'].values - df_data['Predicted_WSE'].values\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def log_scale_transform_timeseries(X):\n",
    "    \n",
    "    shift_value_X = np.abs(np.min(X)) + 1  # Shift X to ensure positive values\n",
    "    \n",
    "    X_shifted = X + shift_value_X\n",
    "    \n",
    "    X_log = np.log(X_shifted)\n",
    "\n",
    "    transformerX = RobustScaler().fit(X_log)\n",
    "    X_trans = transformerX.transform(X_log)\n",
    "    min_max_scalerX = MinMaxScaler().fit(X_trans)\n",
    "    X_trans2 = min_max_scalerX.transform(X_trans)\n",
    "\n",
    "    return X_trans2, transformerX, min_max_scalerX, shift_value_X\n",
    "\n",
    "input_T_flat = input_T.reshape(-1, 1) # stacks each row of input_T in a single column\n",
    "input_T_flat_rescaled, transformerT, min_max_scalerT, shift_value_T  = log_scale_transform_timeseries(input_T_flat)\n",
    "input_T_rescaled = input_T_flat_rescaled.reshape(-1, input_T.shape[1])\n",
    "\n",
    "input_P_flat = input_P.reshape(-1, 1) # stacks each row of input_T in a single column\n",
    "input_P_flat_rescaled, transformerP, min_max_scalerP, shift_value_P  = log_scale_transform_timeseries(input_P_flat)\n",
    "input_P_rescaled = input_P_flat_rescaled.reshape(-1, input_P.shape[1])\n",
    "\n",
    "input_Ev_flat = input_Ev.reshape(-1, 1) # stacks each row of input_T in a single column\n",
    "input_Ev_flat_rescaled, transformerEv, min_max_scalerEv, shift_value_Ev  = log_scale_transform_timeseries(input_Ev_flat)\n",
    "input_Ev_rescaled = input_Ev_flat_rescaled.reshape(-1, input_Ev.shape[1])\n",
    "\n",
    "\n",
    "def log_scale_transform(X, Y):\n",
    "    \n",
    "    shift_value_X = np.abs(np.min(X)) + 1  # Shift X to ensure positive values\n",
    "    shift_value_Y = np.abs(np.min(Y)) + 1  # Shift Y similarly\n",
    "    \n",
    "    X_shifted = X + shift_value_X\n",
    "    Y_shifted = Y + shift_value_Y\n",
    "    \n",
    "    X_log = np.log(X_shifted)\n",
    "    Y_log = np.log(Y_shifted)\n",
    "\n",
    "    transformerX = RobustScaler().fit(X_log)\n",
    "    X_trans = transformerX.transform(X_log)\n",
    "    min_max_scalerX = MinMaxScaler().fit(X_trans)\n",
    "    X_trans2 = min_max_scalerX.transform(X_trans)\n",
    "\n",
    "    transformerY = RobustScaler().fit(np.reshape(Y_log,(-1,1)))\n",
    "    Y_trans = transformerY.transform(np.reshape(Y_log,(-1,1)))\n",
    "    min_max_scalerY = MinMaxScaler().fit(Y_trans)\n",
    "    Y_trans2 = min_max_scalerY.transform(Y_trans)\n",
    "\n",
    "    return X_trans2, Y_trans2, transformerX, transformerY, min_max_scalerX, min_max_scalerY, shift_value_X, shift_value_Y\n",
    "\n",
    "input_point_rescaled_WSE, y_rescaled, transformerPoint, transformerY, min_max_scalerPoint, min_max_scalerY, shift_value_Point, shift_value_Y  = log_scale_transform(input_point_features_WSE, target)\n",
    "\n",
    "def y_to_initial_scale(y, min_max_scaler, transformer, shift):\n",
    "    y = min_max_scaler.inverse_transform(y.reshape(-1, 1))\n",
    "    y = transformer.inverse_transform(y)\n",
    "    return np.exp(y) - shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EILSTMNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomLSTMstack(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_sizes, proj_size = None, dropout_rate = 0):\n",
    "\n",
    "        # input_size is the number of input features\n",
    "        # hidden_sizes must be a list like [6, 3, 2]\n",
    "        # proj_size only is considered for the last layer \n",
    "        # dropout_rate only is not considered for the last layer\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            in_size = input_size if i==0 else hidden_sizes[i-1]\n",
    "            hidden_size = hidden_sizes[i]\n",
    "            is_last = ( i == len(hidden_sizes) - 1) # True or False\n",
    "\n",
    "            lstm = nn.LSTM(\n",
    "                \n",
    "                input_size = in_size,\n",
    "                hidden_size = hidden_size,\n",
    "                num_layers = 1,\n",
    "                proj_size = proj_size if is_last and proj_size is not None else 0,\n",
    "                dropout = dropout_rate if not is_last and len(hidden_sizes) > 1 else 0,\n",
    "                bias = True,\n",
    "                bidirectional=False,\n",
    "                batch_first = True,\n",
    "\n",
    "            )\n",
    "\n",
    "            self.layers.append(lstm)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for i, lstm in enumerate(self.layers):\n",
    "            \n",
    "            x, (hn, cn) = lstm(x) # so in the stacked format, the outputs are passed to the next lstm not the hidden stated\n",
    "\n",
    "        return hn[-1] #shape: (samples, proj_size)\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_input_size, lstm_hidden_sizes, proj_size,\n",
    "                 point_features_shape, mlp_layer_sizes, target_size, dropout_rate = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        # seq_input_size is the number of input features for LSTM\n",
    "        # lstm_hidden_sizes must be a list like [6, 3, 2] for LSTM\n",
    "        # proj_size only is considered for the last layer for LSTM\n",
    "        # dropout_rate only is not considered for the last layer for LSTM and also works for MLP\n",
    "        # point_features_shape is a number of point-based features\n",
    "        # mlp_layer_sizes is a list like [8, 10, 12]\n",
    "        # target_size is a simple number like 1\n",
    "\n",
    "        self.lstm_stack = CustomLSTMstack(\n",
    "\n",
    "            input_size = seq_input_size,\n",
    "            hidden_sizes = lstm_hidden_sizes,\n",
    "            proj_size = proj_size,\n",
    "            dropout_rate = dropout_rate\n",
    "\n",
    "        )\n",
    "\n",
    "        lstm_output_size = proj_size if proj_size is not None else lstm_hidden_sizes[-1]\n",
    "        mlp_input_size = lstm_output_size + point_features_shape\n",
    "\n",
    "        mlp_layers = []\n",
    "        mlp_layer_sizes = [mlp_input_size] + mlp_layer_sizes\n",
    "\n",
    "        for i in range(len(mlp_layer_sizes) - 1):\n",
    "            mlp_layers.append(nn.Linear(mlp_layer_sizes[i], mlp_layer_sizes[i+1]))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            mlp_layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        mlp_layers.append(nn.Linear(mlp_layer_sizes[-1], target_size))\n",
    "\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self,x_seq, x_point):\n",
    "        \n",
    "        lstm_lasthidden = self.lstm_stack(x_seq)\n",
    "        combined = torch.cat([lstm_lasthidden, x_point], dim = 1)\n",
    "\n",
    "        return self.mlp(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def write_hyper_results(params, avg_metrics):\n",
    "\n",
    "\n",
    "    results_csv = f\"hyperparam_results.csv\"\n",
    "\n",
    "    # Convert and write immediately\n",
    "    row = {**params, **avg_metrics}\n",
    "    df = pd.DataFrame([row])\n",
    "\n",
    "    # Append to file\n",
    "    if not os.path.exists(results_csv):\n",
    "        df.to_csv(results_csv, index=False)\n",
    "    else:\n",
    "        df.to_csv(results_csv, mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The hyperparameter search space is set different to what is mentioned in the paper (for faster execution time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_seq = np.stack([input_P_rescaled, input_Ev_rescaled, input_T_rescaled], axis=2)\n",
    "x_seq_tensor = torch.tensor(x_seq, dtype = torch.float32).to(device)\n",
    "x_point = np.hstack([input_point_rescaled_WSE, input_point_features_sensorcodes, input_point_features_DOY])\n",
    "x_point_tensor = torch.tensor(x_point, dtype = torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y_rescaled, dtype=torch.float32).to(device)\n",
    "\n",
    "dataset = TensorDataset(x_seq_tensor, x_point_tensor, y_tensor)\n",
    "\n",
    "# Define search space\n",
    "param_grid = {\n",
    "    'lr': [0.0005, 0.0001, 0.005, 0.001, 0.05, 0.01],\n",
    "    'epochs': [100, 200, 300],\n",
    "    'batch_size': [256, 128, 64, 32, 16, 8],\n",
    "    'lstm_hidden_sizes': [[64, 32, 16, 8, 4], [32, 16, 8, 4], [32, 16]],\n",
    "    'mlp_layer_sizes': [[64, 32, 16, 8, 4], [64, 32, 16], [64, 32, 4], [32, 8, 4]],\n",
    "    'dropout_rate': [0, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "param_names = list(param_grid.keys())\n",
    "\n",
    "# Track best\n",
    "best_params = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid_scores = []\n",
    "\n",
    "total_combinations = len(param_combinations)\n",
    "total_combinations_counter = 1\n",
    "\n",
    "for param_values in param_combinations:\n",
    "    params = dict(zip(param_names, param_values))\n",
    "    # print(f\"\\nTrying params: {params}\")\n",
    "    print(f\"{total_combinations_counter} out of {total_combinations}\")\n",
    "    total_combinations_counter = total_combinations_counter + 1\n",
    "\n",
    "    fold_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=params['batch_size'], shuffle=True)\n",
    "        # print(\"train_loader\", train_loader)\n",
    "        val_loader = DataLoader(val_subset, batch_size=params['batch_size'])\n",
    "        # print(\"val_loader\", val_loader)\n",
    "\n",
    "        model = LSTMRegressor(\n",
    "            seq_input_size=x_seq_tensor.shape[-1],\n",
    "            lstm_hidden_sizes=params['lstm_hidden_sizes'],\n",
    "            proj_size=None,\n",
    "            point_features_shape=x_point_tensor.shape[-1],\n",
    "            mlp_layer_sizes=params['mlp_layer_sizes'],\n",
    "            target_size=y_tensor.shape[-1],\n",
    "            dropout_rate=params['dropout_rate']\n",
    "        ).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        # optimizer = optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(params['epochs']):\n",
    "            model.train()\n",
    "            for x_seq_batch, x_point_batch, y_batch in train_loader:\n",
    "                # print(\"x_seq_batch\", x_seq_batch.shape)\n",
    "                # print(\"x_point_batch\", x_point_batch.shape)\n",
    "                # print(\"y_batch\", y_batch.shape)\n",
    "                x_seq_batch = x_seq_batch.to(device)\n",
    "                x_point_batch = x_point_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                preds = model(x_seq_batch, x_point_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            metrics = {'RMSE': [], 'PCC': []}\n",
    "\n",
    "            all_y_true = []\n",
    "            all_y_pred = []\n",
    "\n",
    "            for x_seq_batch, x_point_batch, y_batch in val_loader:\n",
    "\n",
    "                x_seq_batch = x_seq_batch.to(device)\n",
    "                x_point_batch = x_point_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                preds = model(x_seq_batch, x_point_batch)\n",
    "\n",
    "                all_y_true.append(y_batch.detach().cpu())\n",
    "                all_y_pred.append(preds.detach().cpu())\n",
    "\n",
    "\n",
    "            # ⬇️ Concatenate all batches into one tensor for the fold\n",
    "            y_true_full = torch.cat(all_y_true, dim=0).numpy()\n",
    "            y_pred_full = torch.cat(all_y_pred, dim=0).numpy()\n",
    "\n",
    "            # print(\"y_true\", y_true_full.shape)\n",
    "            # print(\"y_pred\", y_true_full.shape)\n",
    "\n",
    "            # ⬇️ Inverse transform back to real-world scale\n",
    "            y_true_real = y_to_initial_scale(y_true_full[:, 0], min_max_scalerY, transformerY, shift_value_Y).flatten()\n",
    "            y_pred_real = y_to_initial_scale(y_pred_full[:, 0], min_max_scalerY, transformerY, shift_value_Y).flatten()\n",
    "\n",
    "\n",
    "            # Metrics\n",
    "            # ✅ Compute fold-level metrics\n",
    "            rmse = np.sqrt(np.mean((y_pred_real - y_true_real) ** 2))\n",
    "            pcc = np.corrcoef(y_true_real, y_pred_real)[0, 1] * 100\n",
    "\n",
    "            # ✅ Append metrics for this fold\n",
    "            metrics['RMSE'].append(rmse)\n",
    "            metrics['PCC'].append(pcc)\n",
    "\n",
    "        fold_scores.append(metrics)\n",
    "\n",
    "    avg_metrics = {\n",
    "    metric: np.mean([fold[metric][0] for fold in fold_scores])  # unwrap [value]\n",
    "    for metric in ['RMSE', 'PCC']\n",
    "    }\n",
    "\n",
    "    # param_grid_scores.append((params, avg_metrics))\n",
    "    write_hyper_results(params, avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the best model (An example evaluation using 70% data as traning and 30% as test)\n",
    "\n",
    "#### The validation method reported here is not the same as what is reported in the paper. The exact code of the article will be released upon acceptance of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "df = pd.read_csv('hyperparam_results.csv')\n",
    "df_sorted = df.sort_values(by=\"RMSE\", ascending=True)\n",
    "best_params = df_sorted.iloc[0]\n",
    "\n",
    "best_params\n",
    "lr = float(best_params['lr'])\n",
    "epochs = int(best_params['epochs'])\n",
    "batch_size = int(best_params['batch_size'])\n",
    "lstm_hidden_sizes = ast.literal_eval(best_params['lstm_hidden_sizes'])\n",
    "mlp_layer_sizes = ast.literal_eval(best_params['mlp_layer_sizes'])\n",
    "dropout_rate = float(best_params['dropout_rate'])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_seq = np.stack([input_P_rescaled, input_Ev_rescaled, input_T_rescaled], axis=2)\n",
    "x_seq_tensor = torch.tensor(x_seq, dtype = torch.float32).to(device)\n",
    "x_point = np.hstack([input_point_rescaled_WSE, input_point_features_sensorcodes, input_point_features_DOY])\n",
    "x_point_tensor = torch.tensor(x_point, dtype = torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y_rescaled, dtype=torch.float32).to(device)\n",
    "\n",
    "dataset = TensorDataset(x_seq_tensor, x_point_tensor, y_tensor)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_ds, test_ds = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "model = LSTMRegressor(\n",
    "    seq_input_size=x_seq_tensor.shape[-1],\n",
    "    lstm_hidden_sizes=lstm_hidden_sizes,\n",
    "    proj_size=None,\n",
    "    point_features_shape=x_point_tensor.shape[-1],\n",
    "    mlp_layer_sizes=mlp_layer_sizes,\n",
    "    target_size=y_tensor.shape[-1],\n",
    "    dropout_rate=dropout_rate\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for x_seq_batch, x_point_batch, y_batch in train_loader:\n",
    "\n",
    "        x_seq_batch = x_seq_batch.to(device)\n",
    "        x_point_batch = x_point_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        preds = model(x_seq_batch, x_point_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for x_seq_batch, x_point_batch, y_batch in test_loader:\n",
    "\n",
    "        x_seq_batch = x_seq_batch.to(device)\n",
    "        x_point_batch = x_point_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        preds = model(x_seq_batch, x_point_batch)\n",
    "\n",
    "        all_y_true.append(y_batch.detach().cpu())\n",
    "        all_y_pred.append(preds.detach().cpu())\n",
    "\n",
    "\n",
    "    # ⬇️ Concatenate all batches into one tensor for the fold\n",
    "    y_true_full = torch.cat(all_y_true, dim=0).numpy()\n",
    "    y_pred_full = torch.cat(all_y_pred, dim=0).numpy()\n",
    "\n",
    "    # ⬇️ Inverse transform back to real-world scale\n",
    "    y_true_real = y_to_initial_scale(y_true_full[:, 0], min_max_scalerY, transformerY, shift_value_Y).flatten()\n",
    "    y_pred_real = y_to_initial_scale(y_pred_full[:, 0], min_max_scalerY, transformerY, shift_value_Y).flatten()\n",
    "\n",
    "    # Metrics\n",
    "    # ✅ Compute fold-level metrics\n",
    "    rmse = np.sqrt(np.mean((y_pred_real - y_true_real) ** 2))\n",
    "    pcc = np.corrcoef(y_true_real, y_pred_real)[0, 1] * 100\n",
    "\n",
    "    print('RMSE', rmse)\n",
    "    print('PCC', pcc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
