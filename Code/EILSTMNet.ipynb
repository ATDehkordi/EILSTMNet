{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please remind that this code is not the final code of the paper. The final code will be released upon acceptance. The crrent code differs with the main code in some parts for hyperparameter tuning and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part is used to prepare the input data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_alt = pd.read_excel(\"altimetry.xlsx\")   # columns: DateTime, Predicted_WSE, Sensor\n",
    "df_in_situ = pd.read_excel(\"in_situ.xlsx\")  # columns: DateTime, In_Situ_WSE\n",
    "\n",
    "# Convert DateTime to datetime format\n",
    "df_alt['DateTime'] = pd.to_datetime(df_alt['DateTime'])\n",
    "df_in_situ['DateTime'] = pd.to_datetime(df_in_situ['DateTime'])\n",
    "\n",
    "# Sanity check: make sure datetimes match\n",
    "assert all(df_alt['DateTime'] == df_in_situ['DateTime']), \"DateTimes do not match!\"\n",
    "\n",
    "df_alt['Year'] = df_alt['DateTime'].dt.year\n",
    "df_alt['DOY'] = df_alt['DateTime'].dt.dayofyear\n",
    "\n",
    "df_alt['DOY_sin'] = np.sin(2 * np.pi * df_alt['DOY'] / 365)\n",
    "df_alt['DOY_cos'] = np.cos(2 * np.pi * df_alt['DOY'] / 365)\n",
    "\n",
    "def map_sensor(sensor_name):\n",
    "    if sensor_name.startswith(\"S3A\"):\n",
    "        return 0\n",
    "    elif sensor_name.startswith(\"S3B\"):\n",
    "        return 1\n",
    "    elif sensor_name.startswith(\"S6\"):\n",
    "        return 2\n",
    "    elif sensor_name.startswith(\"SWOT\"):\n",
    "        return 3\n",
    "    else:\n",
    "        return -1  # Unknown sensor\n",
    "\n",
    "df_alt['Sensor_Code'] = df_alt['Sensor'].apply(map_sensor)\n",
    "\n",
    "# Check for any -1 values in Sensor_Code\n",
    "unknown_sensors = df_alt[df_alt['Sensor_Code'] == -1]\n",
    "\n",
    "# Display result\n",
    "if not unknown_sensors.empty:\n",
    "    print(\"Unrecognized sensors found:\")\n",
    "    print(unknown_sensors[['DateTime', 'Sensor']])\n",
    "else:\n",
    "    print(\"All sensor names recognized and mapped successfully.\")\n",
    "\n",
    "df_alt['In_Situ_WSE'] = df_in_situ['In_Situ_WSE']\n",
    "\n",
    "df_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Excel files on PC office\n",
    "df_weather = pd.read_excel(\"ERA5.xlsx\")   # columns: DateTime, Predicted_WSE, Sensor\n",
    "\n",
    "df_weather['DATE'] = pd.to_datetime(df_weather['DATE'])\n",
    "df_weather = df_weather.sort_values('DATE').reset_index(drop=True)\n",
    "df_weather = df_weather.set_index('DATE')\n",
    "df_weather\n",
    "\n",
    "n = 30 # length of timeseries\n",
    "\n",
    "temp_series = []\n",
    "prec_series = []\n",
    "evap_series = []\n",
    "valid_flags = []  # to track which rows have full history\n",
    "\n",
    "for timestamp in df_alt['DateTime']:\n",
    "    start_date = timestamp.normalize() - pd.Timedelta(days=n-1)\n",
    "    end_date = timestamp.normalize()\n",
    "\n",
    "    # Slice n days up to and including the day before the timestamp\n",
    "    ts_slice = df_weather.loc[start_date:end_date]\n",
    "\n",
    "    if len(ts_slice) == n:\n",
    "        temp_series.append(ts_slice['Temp_Celsius'].values.tolist())\n",
    "        prec_series.append(ts_slice['Prec'].values.tolist())\n",
    "        evap_series.append(ts_slice['Evap'].values.tolist())\n",
    "        valid_flags.append(True)\n",
    "\n",
    "    else:\n",
    "        temp_series.append(None)\n",
    "        prec_series.append(None)\n",
    "        evap_series.append(None)\n",
    "        valid_flags.append(False)\n",
    "\n",
    "# Add to df_pred\n",
    "df_alt['Temp_series'] = temp_series\n",
    "df_alt['Prec_series'] = prec_series\n",
    "df_alt['Evap_series'] = evap_series\n",
    "df_alt['Valid'] = valid_flags\n",
    "\n",
    "df_alt_valid = df_alt[df_alt['Valid'] == True].reset_index(drop=True) # remove data that had not complete timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alt_valid.to_excel(f\"alt_ERA5_env_{n}days.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "df_data = pd.read_excel(f\"alt_ERA5_env_{n}days.xlsx\")   # columns: DateTime, Predicted_WSE, Sensor\n",
    "\n",
    "input_point_features = np.column_stack([\n",
    "    df_data['Predicted_WSE'].values,\n",
    "    df_data['DOY_sin'].values,\n",
    "    df_data['DOY_cos'].values,\n",
    "    df_data['Sensor_Code'].values\n",
    "])\n",
    "\n",
    "input_T = np.column_stack([\n",
    "    np.array([ast.literal_eval(s) for s in df_data['Temp_series'].values])\n",
    "])\n",
    "\n",
    "input_P = np.column_stack([\n",
    "    np.array([ast.literal_eval(s) for s in df_data['Prec_series'].values])\n",
    "])\n",
    "input_Ev = np.column_stack([\n",
    "    np.array([ast.literal_eval(s) for s in df_data['Evap_series'].values])\n",
    "])\n",
    "target = np.column_stack([\n",
    "    df_data['In_Situ_WSE'].values\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altimetry vs In_situ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(input_point_features[:, 0], 'b') #blue is altimetry\n",
    "plt.plot(target[:, 0], 'r') # red is in-situ\n",
    "\n",
    "\n",
    "\n",
    "rmse_value = np.sqrt(np.mean((input_point_features[:, 0] - target[:, 0])**2))\n",
    "pcc_value = np.corrcoef(target[:, 0], input_point_features[:, 0])[0, 1] * 100  # Pearson Correlation Coefficient\n",
    "print(\"########################################\")\n",
    "print(\"RMSE:\", rmse_value)\n",
    "print(\"PCC :\", pcc_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def log_scale_transform_timeseries(X):\n",
    "    \n",
    "    shift_value_X = np.abs(np.min(X)) + 1  # Shift X to ensure positive values\n",
    "    \n",
    "    X_shifted = X + shift_value_X\n",
    "    \n",
    "    X_log = np.log(X_shifted)\n",
    "\n",
    "    transformerX = RobustScaler().fit(X_log)\n",
    "    X_trans = transformerX.transform(X_log)\n",
    "    min_max_scalerX = MinMaxScaler().fit(X_trans)\n",
    "    X_trans2 = min_max_scalerX.transform(X_trans)\n",
    "\n",
    "    return X_trans2, transformerX, min_max_scalerX, shift_value_X\n",
    "\n",
    "input_T_flat = input_T.reshape(-1, 1) # stacks each row of input_T in a single column\n",
    "input_T_flat_rescaled, transformerT, min_max_scalerT, shift_value_T  = log_scale_transform_timeseries(input_T_flat)\n",
    "input_T_rescaled = input_T_flat_rescaled.reshape(-1, input_T.shape[1])\n",
    "\n",
    "input_P_flat = input_P.reshape(-1, 1) # stacks each row of input_T in a single column\n",
    "input_P_flat_rescaled, transformerP, min_max_scalerP, shift_value_P  = log_scale_transform_timeseries(input_P_flat)\n",
    "input_P_rescaled = input_P_flat_rescaled.reshape(-1, input_P.shape[1])\n",
    "\n",
    "input_Ev_flat = input_Ev.reshape(-1, 1) # stacks each row of input_T in a single column\n",
    "input_Ev_flat_rescaled, transformerEv, min_max_scalerEv, shift_value_Ev  = log_scale_transform_timeseries(input_Ev_flat)\n",
    "input_Ev_rescaled = input_Ev_flat_rescaled.reshape(-1, input_Ev.shape[1])\n",
    "\n",
    "\n",
    "def log_scale_transform(X, Y):\n",
    "    \n",
    "    shift_value_X = np.abs(np.min(X)) + 1  # Shift X to ensure positive values\n",
    "    shift_value_Y = np.abs(np.min(Y)) + 1  # Shift Y similarly\n",
    "    \n",
    "    X_shifted = X + shift_value_X\n",
    "    Y_shifted = Y + shift_value_Y\n",
    "    \n",
    "    X_log = np.log(X_shifted)\n",
    "    Y_log = np.log(Y_shifted)\n",
    "\n",
    "    transformerX = RobustScaler().fit(X_log)\n",
    "    X_trans = transformerX.transform(X_log)\n",
    "    min_max_scalerX = MinMaxScaler().fit(X_trans)\n",
    "    X_trans2 = min_max_scalerX.transform(X_trans)\n",
    "\n",
    "    transformerY = RobustScaler().fit(np.reshape(Y_log,(-1,1)))\n",
    "    Y_trans = transformerY.transform(np.reshape(Y_log,(-1,1)))\n",
    "    min_max_scalerY = MinMaxScaler().fit(Y_trans)\n",
    "    Y_trans2 = min_max_scalerY.transform(Y_trans)\n",
    "\n",
    "    return X_trans2, Y_trans2, transformerX, transformerY, min_max_scalerX, min_max_scalerY, shift_value_X, shift_value_Y\n",
    "\n",
    "input_point_rescaled, y_rescaled, transformerPoint, transformerY, min_max_scalerPoint, min_max_scalerY, shift_value_Point, shift_value_Y  = log_scale_transform(input_point_features, target)\n",
    "\n",
    "def y_to_initial_scale(y, min_max_scaler, transformer, shift):\n",
    "    y = min_max_scaler.inverse_transform(y.reshape(-1, 1))\n",
    "    y = transformer.inverse_transform(y)\n",
    "    return np.exp(y) - shift - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EILSTMNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomLSTMstack(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_sizes, proj_size = None, dropout_rate = 0):\n",
    "\n",
    "        # input_size is the number of input features\n",
    "        # hidden_sizes must be a list like [6, 3, 2]\n",
    "        # proj_size only is considered for the last layer \n",
    "        # dropout_rate only is not considered for the last layer\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            in_size = input_size if i==0 else hidden_sizes[i-1]\n",
    "            hidden_size = hidden_sizes[i]\n",
    "            is_last = ( i == len(hidden_sizes) - 1) # True or False\n",
    "\n",
    "            lstm = nn.LSTM(\n",
    "                \n",
    "                input_size = in_size,\n",
    "                hidden_size = hidden_size,\n",
    "                num_layers = 1,\n",
    "                proj_size = proj_size if is_last and proj_size is not None else 0,\n",
    "                dropout = dropout_rate if not is_last and len(hidden_sizes) > 1 else 0,\n",
    "                bias = True,\n",
    "                bidirectional=False,\n",
    "                batch_first = True,\n",
    "\n",
    "            )\n",
    "\n",
    "            self.layers.append(lstm)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for i, lstm in enumerate(self.layers):\n",
    "            \n",
    "            x, (hn, cn) = lstm(x) # so in the stacked format, the outputs are passed to the next lstm not the hidden stated\n",
    "\n",
    "        return hn[-1] #shape: (samples, proj_size)\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_input_size, lstm_hidden_sizes, proj_size,\n",
    "                 point_features_shape, mlp_layer_sizes, target_size, dropout_rate = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        # seq_input_size is the number of input features for LSTM\n",
    "        # lstm_hidden_sizes must be a list like [6, 3, 2] for LSTM\n",
    "        # proj_size only is considered for the last layer for LSTM\n",
    "        # dropout_rate only is not considered for the last layer for LSTM and also works for MLP\n",
    "        # point_features_shape is a number of point-based features\n",
    "        # mlp_layer_sizes is a list like [8, 10, 12]\n",
    "        # target_size is a simple number like 1\n",
    "\n",
    "        self.lstm_stack = CustomLSTMstack(\n",
    "\n",
    "            input_size = seq_input_size,\n",
    "            hidden_sizes = lstm_hidden_sizes,\n",
    "            proj_size = proj_size,\n",
    "            dropout_rate = dropout_rate\n",
    "\n",
    "        )\n",
    "\n",
    "        lstm_output_size = proj_size if proj_size is not None else lstm_hidden_sizes[-1]\n",
    "        mlp_input_size = lstm_output_size + point_features_shape\n",
    "\n",
    "        mlp_layers = []\n",
    "        mlp_layer_sizes = [mlp_input_size] + mlp_layer_sizes\n",
    "\n",
    "        for i in range(len(mlp_layer_sizes) - 1):\n",
    "            mlp_layers.append(nn.Linear(mlp_layer_sizes[i], mlp_layer_sizes[i+1]))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            mlp_layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        mlp_layers.append(nn.Linear(mlp_layer_sizes[-1], target_size))\n",
    "\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self,x_seq, x_point):\n",
    "        \n",
    "        lstm_lasthidden = self.lstm_stack(x_seq)\n",
    "        combined = torch.cat([lstm_lasthidden, x_point], dim = 1)\n",
    "\n",
    "        return self.mlp(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "#### The fine/tuning method reported here is not the same as what is reported in the paper. The exact code of the article will be released upon acceptance of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def write_hyper_results(params, avg_metrics):\n",
    "\n",
    "\n",
    "    results_csv = f\"alt_env_{n}days_hyperparam_results.csv\"\n",
    "\n",
    "    # Convert and write immediately\n",
    "    row = {**params, **avg_metrics}\n",
    "    df = pd.DataFrame([row])\n",
    "\n",
    "    # Append to file\n",
    "    if not os.path.exists(results_csv):\n",
    "        df.to_csv(results_csv, index=False)\n",
    "    else:\n",
    "        df.to_csv(results_csv, mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_seq = np.stack([input_P_rescaled, input_Ev_rescaled, input_T_rescaled], axis=2)\n",
    "x_seq_tensor = torch.tensor(x_seq, dtype = torch.float32).to(device)\n",
    "x_point_tensor = torch.tensor(input_point_rescaled, dtype = torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y_rescaled, dtype=torch.float32).to(device)\n",
    "\n",
    "dataset = TensorDataset(x_seq_tensor, x_point_tensor, y_tensor)\n",
    "\n",
    "# Define search space\n",
    "param_grid = {\n",
    "    'lr': [0.0005, 0.0001, 0.005, 0.001, 0.05, 0.01],\n",
    "    'epochs': [100, 200, 300],\n",
    "    'batch_size': [256, 128, 64, 32, 16, 8],\n",
    "    'lstm_hidden_sizes': [[64, 32, 16, 8, 4]],\n",
    "    'mlp_layer_sizes': [[64, 32, 16, 8, 4]],\n",
    "    'dropout_rate': [0, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "param_names = list(param_grid.keys())\n",
    "\n",
    "# Track best\n",
    "best_params = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid_scores = []\n",
    "\n",
    "total_combinations = len(param_combinations)\n",
    "total_combinations_counter = 1\n",
    "\n",
    "for param_values in param_combinations:\n",
    "    params = dict(zip(param_names, param_values))\n",
    "    # print(f\"\\nTrying params: {params}\")\n",
    "    print(f\"{total_combinations_counter} out of {total_combinations}\")\n",
    "    total_combinations_counter = total_combinations_counter + 1\n",
    "\n",
    "    fold_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=params['batch_size'], shuffle=True)\n",
    "        # print(\"train_loader\", train_loader)\n",
    "        val_loader = DataLoader(val_subset, batch_size=params['batch_size'])\n",
    "        # print(\"val_loader\", val_loader)\n",
    "\n",
    "        model = LSTMRegressor(\n",
    "            seq_input_size=x_seq_tensor.shape[-1],\n",
    "            lstm_hidden_sizes=params['lstm_hidden_sizes'],\n",
    "            proj_size=None,\n",
    "            point_features_shape=x_point_tensor.shape[-1],\n",
    "            mlp_layer_sizes=params['mlp_layer_sizes'],\n",
    "            target_size=y_tensor.shape[-1],\n",
    "            dropout_rate=params['dropout_rate']\n",
    "        ).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        # optimizer = optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(params['epochs']):\n",
    "            model.train()\n",
    "            for x_seq_batch, x_point_batch, y_batch in train_loader:\n",
    "                # print(\"x_seq_batch\", x_seq_batch.shape)\n",
    "                # print(\"x_point_batch\", x_point_batch.shape)\n",
    "                # print(\"y_batch\", y_batch.shape)\n",
    "                x_seq_batch = x_seq_batch.to(device)\n",
    "                x_point_batch = x_point_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                preds = model(x_seq_batch, x_point_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "\n",
    "            metrics = {'RMSE': [], 'MAPE': [], 'R2': [], 'PCC': []}\n",
    "\n",
    "            all_y_true = []\n",
    "            all_y_pred = []\n",
    "\n",
    "            for x_seq_batch, x_point_batch, y_batch in val_loader:\n",
    "\n",
    "                x_seq_batch = x_seq_batch.to(device)\n",
    "                x_point_batch = x_point_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                preds = model(x_seq_batch, x_point_batch)\n",
    "\n",
    "                all_y_true.append(y_batch.detach().cpu())\n",
    "                all_y_pred.append(preds.detach().cpu())\n",
    "\n",
    "\n",
    "            # ⬇️ Concatenate all batches into one tensor for the fold\n",
    "            y_true_full = torch.cat(all_y_true, dim=0).numpy()\n",
    "            y_pred_full = torch.cat(all_y_pred, dim=0).numpy()\n",
    "\n",
    "            # print(\"y_true\", y_true_full.shape)\n",
    "            # print(\"y_pred\", y_true_full.shape)\n",
    "\n",
    "            # ⬇️ Inverse transform back to real-world scale\n",
    "            y_true_real = y_to_initial_scale(y_true_full[:, 0], min_max_scalerY, transformerY, shift_value_Y).flatten()\n",
    "            y_pred_real = y_to_initial_scale(y_pred_full[:, 0], min_max_scalerY, transformerY, shift_value_Y).flatten()\n",
    "\n",
    "\n",
    "            # Metrics\n",
    "            # ✅ Compute fold-level metrics\n",
    "            rmse = np.sqrt(np.mean((y_pred_real - y_true_real) ** 2))\n",
    "            pcc = np.corrcoef(y_true_real, y_pred_real)[0, 1] * 100\n",
    "\n",
    "            # ✅ Append metrics for this fold\n",
    "            metrics['RMSE'].append(rmse)\n",
    "            metrics['PCC'].append(pcc)\n",
    "\n",
    "        fold_scores.append(metrics)\n",
    "\n",
    "    avg_metrics = {\n",
    "    metric: np.mean([fold[metric][0] for fold in fold_scores])  # unwrap [value]\n",
    "    for metric in ['RMSE', 'PCC']\n",
    "    }\n",
    "\n",
    "    # param_grid_scores.append((params, avg_metrics))\n",
    "    write_hyper_results(params, avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the best model\n",
    "\n",
    "#### The validation method reported here is not the same as what is reported in the paper. The exact code of the article will be released upon acceptance of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "df = pd.read_csv(f'alt_env_{n}days_hyperparam_results.csv')\n",
    "df_sorted = df.sort_values(by=\"RMSE\", ascending=True)\n",
    "best_params = df_sorted.iloc[0]\n",
    "\n",
    "best_params\n",
    "lr = float(best_params['lr'])\n",
    "epochs = int(best_params['epochs'])\n",
    "batch_size = int(best_params['batch_size'])\n",
    "lstm_hidden_sizes = ast.literal_eval(best_params['lstm_hidden_sizes'])\n",
    "mlp_layer_sizes = ast.literal_eval(best_params['mlp_layer_sizes'])\n",
    "dropout_rate = float(best_params['dropout_rate'])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_seq = np.stack([input_P_rescaled, input_Ev_rescaled, input_T_rescaled], axis=2)\n",
    "x_seq_tensor = torch.tensor(x_seq, dtype = torch.float32).to(device)\n",
    "x_point_tensor = torch.tensor(input_point_rescaled, dtype = torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y_rescaled, dtype=torch.float32).to(device)\n",
    "\n",
    "dataset = TensorDataset(x_seq_tensor, x_point_tensor, y_tensor)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_ds, test_ds = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "model = LSTMRegressor(\n",
    "    seq_input_size=x_seq_tensor.shape[-1],\n",
    "    lstm_hidden_sizes=lstm_hidden_sizes,\n",
    "    proj_size=None,\n",
    "    point_features_shape=x_point_tensor.shape[-1],\n",
    "    mlp_layer_sizes=mlp_layer_sizes,\n",
    "    target_size=y_tensor.shape[-1],\n",
    "    dropout_rate=dropout_rate\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for x_seq_batch, x_point_batch, y_batch in train_loader:\n",
    "\n",
    "        x_seq_batch = x_seq_batch.to(device)\n",
    "        x_point_batch = x_point_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        preds = model(x_seq_batch, x_point_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "\n",
    "    for x_seq_batch, x_point_batch, y_batch in test_loader:\n",
    "\n",
    "        x_seq_batch = x_seq_batch.to(device)\n",
    "        x_point_batch = x_point_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        preds = model(x_seq_batch, x_point_batch)\n",
    "\n",
    "        all_y_true.append(y_batch.detach().cpu())\n",
    "        all_y_pred.append(preds.detach().cpu())\n",
    "\n",
    "\n",
    "    # ⬇️ Concatenate all batches into one tensor for the fold\n",
    "    y_true_full = torch.cat(all_y_true, dim=0).numpy()\n",
    "    y_pred_full = torch.cat(all_y_pred, dim=0).numpy()\n",
    "\n",
    "    # ⬇️ Inverse transform back to real-world scale\n",
    "    y_true_real = y_to_initial_scale(y_true_full[:, 0], min_max_scalerY, transformerY, shift_value_Y).flatten()\n",
    "    y_pred_real = y_to_initial_scale(y_pred_full[:, 0], min_max_scalerY, transformerY, shift_value_Y).flatten()\n",
    "\n",
    "    # Metrics\n",
    "    # ✅ Compute fold-level metrics\n",
    "    rmse = np.sqrt(np.mean((y_pred_real - y_true_real) ** 2))\n",
    "    pcc = np.corrcoef(y_true_real, y_pred_real)[0, 1] * 100\n",
    "\n",
    "    print('RMSE', rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
